<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Resume - Start Bootstrap Theme</title>

  <!-- Bootstrap core CSS -->
  <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet">
  <link href="../vendor/fontawesome-free/css/all.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="resume.css" rel="stylesheet">
   <!-- Bootstrap core JavaScript -->
  <script src="../vendor/jquery/jquery.min.js"></script>
  <script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="../vendor/jquery-easing/jquery.easing.min.js"></script>

</head>

<body id="page-top">
  <div class="container-fluid">
    <section class="resume-section   justify-items-center" id="about">
      <div>
        <h1>Understanding Neural Networks - Part 1</h1>
        <div class="subheading mb-5" style="margin-top: 10px">Get deep into how neural networks really work behind the scenes</div>
          <div class="resume-content">
            <p>Neural networks have revolutionized how we solve complex problems that have been unsolvable using other techniques. Even though we can get them to work, due to their complex mathematical architecture it is difficult to understand why they work (or more importantly why they don’t work). Google’s Ali Rahimi compared machine learning to alchemy. Even though Yann Lecun strongly disagreed with this statement, he did state in a recent interview “There are all kinds of tricks to get backprop to work, and it’s still a bit of a black art — but now we have a recipe. If you follow the recipe, it’s going to work every time.” In this series of blog articles I will try to explain the inner working of neural networks as to give a better intuition to what is happening behind the scenes.</p>
           <p>This series of blog posts will be split into two parts. The first set of blog posts “Understanding Neural Networks” will give a better understanding of the neural network problem domain. We will give a full explanation of what the neural network is doing and under which conditions it is operating. We will do what is seldom done and show that a neural network is actually an approximator. We will show that the neural network is trying to emulate the function that is generating the labels for a set of given data points. Finally we will give examples into where and why neural networks fail and give ideas on what can be done.</p>
           <p>In the second part of the series we will dive into the mathematics of the neural networks. We will show how neural networks split up the input space (or more exactly chops it up). We will see how the points are remapped in each layer and we will even calculate the final function and see the different constraints. We will be working on small simple fully connected networks with ReLU as the activation function for classification problems. Even so will show that some of the same concepts work with more complex architectures and activation functions. So lets get started!!</p>
         </div>
       </div>
       <hr class="m-0">
       <div>
        <h2 class="mb-5" style="margin-top: 30px">Neural network as an approximator</h2>
          <div class="resume-content">
           <p>Since neural networks design was inspired by the human brain, it is easy to forget that at the end of the day a neural network is a mathematical function. Here we will look at neural networks a little bit differently. Instead of looking at the individual neuron and what it has learnt we will take a “zoom out” approach and look at the entire neural network function and how it spans the entire input space. This approach is very input space oriented. This means that we focus allot more on different regions of the input space. To be even more exact we will focus on how the data points are distributed in the input space. This will hopefully give the reader better insights and intuition in neural networks.</p>
            <p>So we claim that a neural network is an approximator. What does this mean? This means that the neural network is trying to approximate a function. The question is what function is it trying to approximate? Under what conditions? How is it doing it? What are the limitations? In order to find answers to these questions we have to better understand the problem domain in which we are operating. We will do so with an example.</p>
            <p>Let’s say that John is on vacation in Paris. During the day he is snapping away with his camera and during the evening he is dining out so he doesn’t have time to take pictures. After his week long vacation he has a significant number of photos of Paris in the daytime which he decides to share with the world by uploading the pictures online.</p>

            <img src="https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image1.png?alt=media&token=828daa1f-8ae9-4cdf-ae53-d4b889a17975" style="width: 100%;padding: 20px">
            <p>Let us look at the photos as a computer would see them. Let’s say John’s camera isn’t great and the photos are 320x240 grayscale images. So each photo is a single 76800 (=320x240) long vector with values between 0 and 255. How would these points look in the input space? If the camera was only 3 pixels long than our input space would be a 3 axis input space. Then each picture would be a single point inside of a 255x255x255 cube in the input space.</p>
            <img src="https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image2.png?alt=media&token=483ad655-8010-48e1-8818-a3508c740f7d" style="width: 100% padding:20px">
            <p>If this is the case, we can imagine that in our case each one of John’s pictures is a single point inside of a 76800 dimensional hyper-cube.</p>
            <p>We should notice something important. In total there is a finite number of possible pictures (256⁷⁶⁸⁰⁰ to be exact). Out of all these points some, if not most, of these points are not even real pictures but rather random noise. These are “pictures” that can’t really be taken with John’s camera. In other words if we talk about each picture being a sample in the input space, then we can be sure that it is physically impossible to capture these images (sample these points) using John’s camera.</p>
            <p>If we look at just one of the pictures taken by John, as we showed before, this picture is a point in the input space. What if we look at the the points adjacent to the that point, or that are really close to it? We will see that the pictures that they represent only differ a bit from the original picture. A person looking at the pictures will say they are essentially the same picture. Thus if we were to build a neural network we would expect the network to give the same results for all the points around that point (as long as we don’t wonder too far away off). So points that are close enough to each other in the input space are essentially the same picture. The opposite isn’t true. If we have two pictures, where the second picture is the same as the first picture just all the points are moved one pixel to the right. They are essentially the same picture but there points are not necessarily close in the input space.</p>
            <p>Lastly we will notice that John is selectively only sampling various regions of the input space. His pictures are sampling Paris in the daytime (probably many pixels will be light high values). These points are probably pretty far away from pictures of Beijing at night (most pixels will be dark low values).</p>
            <p>We can split the 256⁷⁶⁸⁰⁰ theoretical pictures into three groups. The first group is regions of the input space which are random noise pictures that a camera can’t produce. These are areas of the input space which will never sample neither during the training nor during the inference stage since it is impossible for a camera to sample them. The remaining areas that hold legal images we can further split into two groups. Firstly is regions of the input space which are close to the pictures John took of Paris during the day. The last group is all the rest of input space. Pictures of Beijing during the night will belong to this group. It will come back later on.</p>
         </div>
       </div>
       <hr class="m-0">
       <div>
         <h2 class="mb-5" style="margin-top: 30px">Creating a data set</h2>
          <div class="resume-content">
            <p>We decide to use John’s pictures to build a neural network that does something. In order to do this, pictures aren’t enough. We need a labelled data set. Let’s say we want to build a neural network is able to classify if there is a cat in the picture. In order to create this data set we hire 1 million people. We ask EACH person to tag EVERY picture if there is a cat in the picture. So we get a data set with 1 million classification result for EVERY one of John’s pictures. The 1 million labels for a single image will not necessarily be the same. This could be due to the fact that someone may have mislabeled the image, but it could be due to the fact that the people we hired may not necessarily agree with each other. Maybe the cat is hidden or it is difficult to determine that there is a cat in the picture. Maybe only the edge of the cat’s tail is visible in the image. In such a case it becomes more of a philosophical question: Is a cat tail consider a cat in the picture?</p>
            <img src="https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image3.png?alt=media&token=17312747-7a20-46a3-9780-2582521d9ebc" style="width: 100%; padding: 20px">
            <p>We could have just as easily asked the million people any other question and created a totally different data set that could be used to teach another neural network something else altogether.</p>
            <p>Finally we will use this data set to build a neural network. The purpose of the neural network is to essentially replace the 1 million people. That means that if we have a new picture and feed it through the neural network the answer we should get should be the same as the average of we asked 1 million people if there was a cat in the picture.</p>
            <img src="https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image4.png?alt=media&token=f49a8da1-4ec5-41a4-9e1d-917bdf6af76d" style="width: 100%; padding:20px">
            <p>Let’s dive in a little deeper how we are building this neural network. We take the data set and perform the training using back propagation. So every time we go through all the data points and we are changing the weights of our neural network so that the output of the network will be closer to the data point’s output. We only look at one of John’s pictures. We have 1 million data point entries for that particular input point (picture). All the entries that classify that image as containing a cat will try to pull the output of our neural network up towards 1 for that particular input, and all the entries that classify no cat will try to bring it down to 0. After running back propagation for enough epochs the resulting neural network output for that particular picture in the input should be the average of all the 1 million classifiers. So if 99% of the classifiers agree that there is a cat in the picture the output of the neural network should ideally be around 0.99. In practice this may not be the case and the neural network could still give us good results but ideally this is what we are looking for.</p>
            <p>This is all nice, but in real life we can’t have 1 million people tag every image. That is true, but this theoretical example is giving us some basic intuition into what is going on. How is a real data set created? Let’s imagine we have a function that is spread all over the input space. This function answers the following question for each and every possible picture in the input space (which includes 256⁷⁶⁸⁰⁰ possible theoretical pictures). For each picture it answers: “If I were to give THIS image to one of the people doing the classification what is the probability that they would return that there is a cat in the picture?” This function is a very important function. We will eventually see that this in fact is the function that the neural network is trying to emulate, at least in certain parts of the input space.</p>
            <p>But how does this function help us create a labeled data set? In order to create the data set, we sample this function at certain points in the input space. In our example these points are John’s pictures. For each input point we get a probability. We use that probability to draw a label for that data point. That is how we eventually build a complete data set. In order to convince you more that this probability function really exists here is another way of looking at how the training data is created. In order to create a label for a certain picture, we randomly pick one of the million people we had labeling the data. We then take the result of their classification as the label for that picture. Randomly drawing a person and using their label is essentially the same as finding the probability of getting a certain label and then drawing a label according to that probability. There are a few constraints on this probabilistic function, in order for it to even be possible to solve the classification problem using neural networks. One of the constraints is that this function has to be a relatively smooth, at least in the areas of the input space where we will be sampling. This is a necessity since if the function is too erratic knowing the labels for a certain point will not help us understand the labels of adjacent points or points which are in close proximity. In such a case we will say that the problem doesn’t generalize well. This is not the only constraint, there are more which we will cover in future posts.</p>
          </div>
       </div>
       <hr class="m-0">
       <div>
         <h2 class="mb-5"style="margin-top: 30px">Using the dataset to approximate</h2>
          <div class="resume-content">
            <p>We then use this data set to try to approximate this probabilistic function. Since we are assuming that this probabilistic function is relatively smooth, the images, which are scattered in the input space, together with their labels, will help us rebuild this probabilistic function. To reconstruct the function we use a technique called curve fitting (which is what back propagation is essentially doing). We will cover more of this in the following blog post.</p>
            <p>I would like to note that the input distribution, the pictures John is taking, is totally independent of the classification and everything else we are doing. John is independent to take any photos he likes regardless of anything. We can already start to see where problems might arise. Let’s say that John continued with his vacation and flew to Beijing, China. While in Beijing he toured one of Beijing’s night markets. As before he took a bunch of pictures and uploaded them to the internet. Let’s say we decided to take these pictures and test our neural network. We have already established that the pictures of Beijing during the night are probably in a very different from the pictures of Paris during the day, and occupy different areas of the input space. What we will see is that the neural network will give really bad result in this part of the input space. The resulting output may even be out of the 0–1 range, meaning we might get a probability that is higher than one or negative which doesn’t make sense. We will understand why this is in the next blog post. But the fact that the neural network give bad results does make sense. From the data the neural network is fed in the training stage, it is theoretically impossible for the neural network to give reliable results outside of its input distribution. Let us explain. The neural network only sees examples of pictures and labels. It doesn’t know the question we asked the people that were doing the classification. The question we used to train the neural network was “is there a cat in the picture?”, but it could have easily been “is there a cat in the picture during the day?”. These two questions would have generated the same data set for John’s Paris day time pictures. If we build a neural network from this data set, and then queried it on Beijing’s night time pictures what answer will the neural network return? Since the neural network has no way of knowing the classification question from the given data, it has no way of returning a correct answer.</p>
            <p>This is giving us a clue that contrary to popular belief, the neural network isn’t learning in general what a cat is. Instead the neural network is trying to figure out if a given image is similar to another images it has seen in the training data. It is using these images, which are relatively close to it in the input space in order to try to figure how to classify that point. That is why if we try to classify points that are far away from our input distribution we will find that the neural network doesn’t work well.</p>
            <p>If you remember we talked about the input space being split up into three groups: regions of the input space which are random noise, regions which are the same close to the training images and all the rest. In regions that are of random noise, we don’t really care what result our neural network will give us since we assume that we will not be sampling from these regions. Then we had the regions that were not close to the training data. These are data points that are way out of the areas of the input distribution. These are regions where we have to be careful because the network will return a result but we have no way of knowing if the result is correct. Using the neural network to evaluate the function is these regions is called extrapolation and the results are unreliable. For the regions of the input space where we had our training images (areas that are part of the input distribution), we should get decent results since we build the neural network to preform well in these areas. Even so there may be parts of these regions where the neural network may not preform well. This has to do with the input distribution. For instance if there are areas where we have a very small number of points in our input distribution. You can image that train a network with John’s Paris pictures together with a single picture of Beijing’s night market. This new neural network will not, all of a sudden, perform well the Beijing night market pictures.</p>
          </div>
        </div>
          <hr class="m-0">
       <div>
         <h2 class="mb-5" style="margin-top: 30px">Summing things up</h2>
          <div class="resume-content">
            <p>We started off with an input space dictated by our features (in our example all the possible pictures). On top of this input space there is an unknown probabilistic function that answers the question “For a given input what is the probability that it will be classified as (input whatever question)?”. This is the function that we ideally want our neural network to emulate, at least for the areas of the input space where we will be sampling. We then sample this function in the points of the input space according to a given input distribution (John’s pictures). For each of these points we draw a label according to this probability given by this function. That is how we derive a data set. We then take this data set and use it to train our neural network hoping that we get back a function that is close enough to the unknown probabilistic function in the areas of the input space of our given input distribution.</p>
            <div class="subheading mb-5">See you in the next part. Stay tuned !!</div>
          </div>
        </div>
    </section>
  </div>
</body>
</html>


<!-- 
<hr class="m-0">
  <div class="container-fluid">
  <section class="resume-section justify-content-center" id="experience" style="height: 100%">
    <div class="w-100">
        <h2 class="mb-5">Neural network as an approximator</h2>
        <div class="resume-item d-flex flex-column flex-md-row justify-content-between mb-5">
          <div class="resume-content">
            <p>Since neural networks design was inspired by the human brain, it is easy to forget that at the end of the day a neural network is a mathematical function. Here we will look at neural networks a little bit differently. Instead of looking at the individual neuron and what it has learnt we will take a “zoom out” approach and look at the entire neural network function and how it spans the entire input space. This approach is very input space oriented. This means that we focus allot more on different regions of the input space. To be even more exact we will focus on how the data points are distributed in the input space. This will hopefully give the reader better insights and intuition in neural networks.</p>
            <p>So we claim that a neural network is an approximator. What does this mean? This means that the neural network is trying to approximate a function. The question is what function is it trying to approximate? Under what conditions? How is it doing it? What are the limitations? In order to find answers to these questions we have to better understand the problem domain in which we are operating. We will do so with an example.</p>
            <p>Let’s say that John is on vacation in Paris. During the day he is snapping away with his camera and during the evening he is dining out so he doesn’t have time to take pictures. After his week long vacation he has a significant number of photos of Paris in the daytime which he decides to share with the world by uploading the pictures online.</p>

            <img src="https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image1.png?alt=media&token=828daa1f-8ae9-4cdf-ae53-d4b889a17975" style="width: 100%;height: 50%">
            <p>Let us look at the photos as a computer would see them. Let’s say John’s camera isn’t great and the photos are 320x240 grayscale images. So each photo is a single 76800 (=320x240) long vector with values between 0 and 255. How would these points look in the input space? If the camera was only 3 pixels long than our input space would be a 3 axis input space. Then each picture would be a single point inside of a 255x255x255 cube in the input space.</p>
            <img src="https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image2.png?alt=media&token=483ad655-8010-48e1-8818-a3508c740f7d">
            <p>If this is the case, we can imagine that in our case each one of John’s pictures is a single point inside of a 76800 dimensional hyper-cube.</p>
            <p>We should notice something important. In total there is a finite number of possible pictures (256⁷⁶⁸⁰⁰ to be exact). Out of all these points some, if not most, of these points are not even real pictures but rather random noise. These are “pictures” that can’t really be taken with John’s camera. In other words if we talk about each picture being a sample in the input space, then we can be sure that it is physically impossible to capture these images (sample these points) using John’s camera.</p>
            <p>If we look at just one of the pictures taken by John, as we showed before, this picture is a point in the input space. What if we look at the the points adjacent to the that point, or that are really close to it? We will see that the pictures that they represent only differ a bit from the original picture. A person looking at the pictures will say they are essentially the same picture. Thus if we were to build a neural network we would expect the network to give the same results for all the points around that point (as long as we don’t wonder too far away off). So points that are close enough to each other in the input space are essentially the same picture. The opposite isn’t true. If we have two pictures, where the second picture is the same as the first picture just all the points are moved one pixel to the right. They are essentially the same picture but there points are not necessarily close in the input space.</p>
            <p>Lastly we will notice that John is selectively only sampling various regions of the input space. His pictures are sampling Paris in the daytime (probably many pixels will be light high values). These points are probably pretty far away from pictures of Beijing at night (most pixels will be dark low values).</p>
            <p>We can split the 256⁷⁶⁸⁰⁰ theoretical pictures into three groups. The first group is regions of the input space which are random noise pictures that a camera can’t produce. These are areas of the input space which will never sample neither during the training nor during the inference stage since it is impossible for a camera to sample them. The remaining areas that hold legal images we can further split into two groups. Firstly is regions of the input space which are close to the pictures John took of Paris during the day. The last group is all the rest of input space. Pictures of Beijing during the night will belong to this group. It will come back later on.</p>
          </div>
        </div>
      </div>
    </section>
   -->
