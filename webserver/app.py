import pymongo
from pymongo import MongoClient
from flask import Flask
from flask import Flask, url_for,jsonify
from flask import request
import json
from flask_cors import CORS
app = Flask(__name__)
CORS(app)


"""
reviews array of dictionaries
{
    "username":"",
    "timestamp":"",
    "message":"",
    "pos":,
    "neg":0,
    "neu":0 
}

Course_metadata.post1 = {
    "_id":"ML001",
    "course_name":"",
    "course_description":"",
    "author":"",
    "rating":null,
    "reviews":[],
    "duration":3,
    "part1":{"summ":"","title":""},
    "part2":{"summ":"","title":""},
    "part3":{"summ":"","title":""},
    "part4":{"summ":"","title":""}
}


Course.course_post1 = {
    "_id":"ML001",
    "part1":"",
    "part2":"",
    "part3":"",
    "part4":""
}

ImageCollection.image_post1 = {
    "_id":"ML001",
    "part1":[],
    "part2":[]
    "part3":[]
    "part4":[]
}

"""
"""
course_post1 = {
    "_id":"ML001",
    "part1": 
    "<section class=\"resume-section   justify-items-center\" id=\"about\">      <div>        <h1>Understanding Neural Networks - Part 4</h1>        <div class=\"subheading mb-5\" style=\"margin-top: 10px\">Get deep into how neural networks really work behind the scenes</div>          <div class=\"resume-content\">            <p>This is the final part of the neural network tutorial.</p>         </div>       </div>       <hr class=\"m-0\">       <div>        <h2 class=\"mb-5\" style=\"margin-top: 30px\">10 Applications of Artificial Neural Networks in Natural Language Processing</h2>          <div class=\"resume-content\">           <p>Since artificial neural networks allow modeling of nonlinear processes, they have turned into a very popular and useful tool for solving many problems such as classification, clustering, regression, pattern recognition, dimension reduction, structured prediction, machine translation, anomaly detection, decision making, visualization, computer vision, and others. This wide range of abilities makes it possible to use artificial neural networks in many areas. In this article, we discuss applications of artificial neural networks in Natural Language Processing tasks (NLP).</p>            <p>NLP includes a wide set of syntax, semantics, discourse, and speech tasks. We will describe prime tasks in which neural networks demonstrated state-of-the-art performance.</p>         </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">1. Text Classification and Categorization</h2>          <div class=\"resume-content\">            <p>Text classification is an essential part in many applications, such as web searching, information filtering, language identification, readability assessment, and sentiment analysis. Neural networks are actively used for these tasks.</p>            <p>In Convolutional Neural Networks for Sentence Classification by Yoon Kim, a series of experiments with Convolutional Neural Networks (CNN) built on top of word2vec was presented. The suggested model was tested against several benchmarks. In Movie Reviews (MR) and Customer Reviews (CR), the task was to detect positive/negative sentiment. In Stanford Sentiment Treebank (SST-1), there were already more classes to predict: very positive, positive, neutral, negative, very negative. In Subjectivity data set (Subj), sentences were classified into two types, subjective or objective. In TREC the goal was to classify a question into six question types (whether the question is about person, location, numeric information, etc.) The results of numerous tests described in the paper show that after little tuning of hyperparameters the model performs excellent suggesting that the pre-trained vectors are universal feature extractors and can be utilized for various classification tasks. </p>            <p>The article Text Understanding from Scratch by Xiang Zhang and Yann LeCun shows that it’s possible to apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts with help of temporal Convolutional Networks (ConvNets) (CNN). Here, the authors assert that ConvNets can achieve excellent performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language [2]. To prove their assertion several experiments were conducted. The model was tested on the DBpedia ontology classification data set with 14 classes (company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film, written work). The results indicate both good training (99.96%) and testing (98.40 %) accuracy, with some improvement from thesaurus augmentation. In addition, the sentiment analysis test was performed on the Amazon Review data set. In this study, the researchers constructed a sentiment polarity data set with two negative and two positive labels. The result is 97.57% training accuracy and 95.07% testing accuracy. The model was also tested on Yahoo! Answers Comprehensive Questions and Answers data set with 10 classes (Society & Culture, Science & Mathematics, Health, Education & Reference, Computers & Internet, Sports, Business & Finance, Entertainment & Music, Family & Relationships, Politics & Government) and on AG’s corpus where the task was a news categorization into four categories (World, Sports, Business, Sci/Tech.). Obtained results confirm that to achieve good text understanding ConvNets require a large corpus in order to learn from scratch.</p>            <p>Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao introduced recurrent convolutional neural networks for text classification without human-designed features in their document Recurrent Convolutional Neural Networks for Text Classification [3]. The team tested their model using four data sets: 20Newsgroup (with four categories such as computers, politics, recreation, and religion), Fudan Set (a Chinese document classification set that consists of 20 classes, including art, education, and energy), ACL Anthology Network (with five languages: English, Japanese, German, Chinese, and French), and Sentiment Treebank (with Very Negative, Negative, Neutral, Positive, and Very Positive labels). After testing, the model was compared to existing text classification methods like Bag of Words, Bigrams + LR, SVM, LDA, Tree Kernels, RecursiveNN, and CNN. It turned out that neural network approaches outperform traditional methods for all four data sets, and the proposed model outperforms CNN and RecursiveNN.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">2. Named Entity Recognition (NER)</h2>          <div class=\"resume-content\">            <p>The main task of named entity recognition (NER) is to classify named entities, such as Guido van Rossum, Microsoft, London, etc., into predefined categories like persons, organizations, locations, time, dates, and so on. Many NER systems were already created, and the best of them use neural networks.</p>            <p>In the paper, Neural Architectures for Named Entity Recognition, two models for NER were proposed. The models require character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora [4]. Numerous tests were carried on using different data sets like CoNLL-2002 and CoNLL-2003 in English, Dutch, German, and Spanish languages. The team concluded that without a requirement of any language-specific knowledge or resources, such as gazetteers, their models show state-of-the-art performance in NER.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">3. Part-of-Speech Tagging</h2>          <div class=\"resume-content\">            <p>Part-of-speech (POS) tagging has many applications including parsing, text-to-speech conversion, information extraction, and so on. In the work, Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network a recurrent neural network with word embedding for part-of-speech (POS) tagging task is presented [5]. The model was tested on the Wall Street Journal data from Penn Treebank III data set and achieved a performance of 97.40% tagging accuracy.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">4. Semantic Parsing and Question Answering</h2>          <div class=\"resume-content\">            <p>Question Answering systems automatically answer different types of questions asked in natural languages including definition questions, biographical questions, multilingual questions, and so on. Neural networks usage makes it possible to develop high performing question answering systems.</p>            <p>In Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao described the developed semantic parsing framework for question answering using a knowledge base. Authors say their method uses the knowledge base at an early stage to prune the search space and thus simplifies the semantic matching problem [6]. It also applies an advanced entity linking system and a deep convolutional neural network model that matches questions and predicate sequences. The model was tested on WebQuestions data set, and it outperforms previous methods substantially.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">5. Paraphrase Detection</h2>          <div class=\"resume-content\">            <p>Paraphrase detection determines whether two sentences have the same meaning. This task is especially important for question answering systems since there are many ways to ask the same question.</p>            <p>Detecting Semantically Equivalent Questions in Online User Forums suggests a method for identifying semantically equivalent questions based on a convolutional neural network. The experiments are performed using the Ask Ubuntu Community Questions and Answers (Q&A) site and Meta Stack Exchange data. It was shown that the proposed CNN model achieves high accuracy especially when the words embedded are pre-trained on in-domain data. The authors compared their model’s performance with Support Vector Machines and a duplicate detection approach. They demonstrated that their CNN model outperforms the baselines by a large margin.</p>            <p>In the study, Paraphrase Detection Using Recursive Autoencoder, a novel recursive autoencoder architecture is presented. It learns phrasal representations using recursive neural networks. These representations are vectors in an n-dimensional semantic space where phrases with similar meanings are close to each other [8]. For evaluating the system, the Microsoft Research Paraphrase Corpus and English Gigaword Corpus were used. The model was compared to three baselines, and it outperforms them all.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">6. Language Generation and Multi-document Summarization</h2>          <div class=\"resume-content\">            <p>Natural language generation has many applications such as automated writing of reports, generating texts based on analysis of retail sales data, summarizing electronic medical records, producing textual weather forecasts from weather data, and even producing jokes.</p>            <p>In a recent paper, Natural Language Generation, Paraphrasing and Summarization of User Reviews with Recurrent Neural Networks, researchers describe a recurrent neural network (RNN) model capable of generating novel sentences and document summaries. The paper described and evaluated a database of 820,000 consumer reviews in the Russian language. The design of the network permits users control of the meaning of generated sentences. By choosing sentence-level features vector, it is possible to instruct the network; for example, “Say something good about a screen and sound quality in about ten words” [9]. The ability of language generation allows production of abstractive summaries of multiple user reviews that often have reasonable quality. Usually, the summary report makes it possible for users to quickly obtain the information contained in a large cluster of documents.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">7. Machine Translation</h2>          <div class=\"resume-content\">            <p>Machine translation software is used around the world despite its limitations. In some domains, the quality of translation is not good. To improve the results researchers try different techniques and models, including the neural network approach. The purpose of Neural-based Machine Translation for Medical Text Domain study is to inspect the effects of different training methods on a Polish-English machine translation system used for medical data. To train neural and statistical network-based translation systems The European Medicines Agency parallel text corpus was used. It was demonstrated that a neural network requires fewer resources for training and maintenance. In addition, a neural network often substituted words with other words occurring in a similar context.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">8. Speech Recognition</h2>          <div class=\"resume-content\">            <p>Speech recognition has many applications, such as home automation, mobile telephony, virtual assistance, hands-free computing, video games, and so on. Neutral networks are widely used in this area.</p>            <p>In Convolutional Neural Networks for Speech Recognition, scientists explain how to apply CNNs to speech recognition in a novel way, such that the CNN’s structure directly accommodates some types of speech variability like varying speaking rate [11]. TIMIT phone recognition and a large-vocabulary voice search tasks were used.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">9. Character Recognition</h2>          <div class=\"resume-content\">            <p>Character Recognition systems also have numerous applications like receipt character recognition, invoice character recognition, check character recognition, legal billing document character recognition, and so on. The article Character Recognition Using Neural Network presents a method for the recognition of handwritten characters with 85% accuracy </p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">10. Spell Checking</h2>          <div class=\"resume-content\">            <p>Most text editors let users check if their text contains spelling mistakes. Neural networks are now incorporated into many spell-checking tools.</p>            <p>In Personalized Spell Checking using Neural Networks a new system for detecting misspelled words was proposed. This system is trained on observations of the specific corrections that a typist makes [13]. It outwits many of the shortcomings that traditional spell-checking methods have.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">Summary</h2>          <div class=\"resume-content\">            <p>In this article, we described Natural Language Processing problems that can be solved using neural networks. As we showed, neural networks have many applications such as text classification, information extraction, semantic parsing, question answering, paraphrase detection, language generation, multi-document summarization, machine translation, and speech and character recognition. In many cases, neural networks methods outperform other methods.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">Resources</h2>          <div class=\"resume-content\">            <p>http://www.aclweb.org/anthology/D14-1181</p>            <p>https://arxiv.org/pdf/1502.01710.pdf</p>            <p>https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552</p>            <p>http://www.aclweb.org/anthology/N16-1030</p>            <p>https://arxiv.org/pdf/1510.06168.pdf</p>            <p>http://www.aclweb.org/anthology/P15-1128</p>            <p>https://www.aclweb.org/anthology/K15-1013</p>            <p>https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf</p>            <p>http://www.meanotek.ru/files/TarasovDS(2)2015-Dialogue.pdf</p>            <p>http://www.sciencedirect.com/science/article/pii/S1877050915025910</p>          </div>       </div>    </section>",
    "part2":"<section class=\"resume-section   justify-items-center\" id=\"about\">      <div>        <h1>Understanding Neural Networks - Part 1</h1>        <div class=\"subheading mb-5\" style=\"margin-top: 10px\">Get deep into how neural networks really work behind the scenes</div>          <div class=\"resume-content\">            <p>Neural networks have revolutionized how we solve complex problems that have been unsolvable using other techniques. Even though we can get them to work, due to their complex mathematical architecture it is difficult to understand why they work (or more importantly why they don’t work). Google’s Ali Rahimi compared machine learning to alchemy. Even though Yann Lecun strongly disagreed with this statement, he did state in a recent interview “There are all kinds of tricks to get backprop to work, and it’s still a bit of a black art — but now we have a recipe. If you follow the recipe, it’s going to work every time.” In this series of blog articles I will try to explain the inner working of neural networks as to give a better intuition to what is happening behind the scenes.</p>           <p>This series of blog posts will be split into two parts. The first set of blog posts “Understanding Neural Networks” will give a better understanding of the neural network problem domain. We will give a full explanation of what the neural network is doing and under which conditions it is operating. We will do what is seldom done and show that a neural network is actually an approximator. We will show that the neural network is trying to emulate the function that is generating the labels for a set of given data points. Finally we will give examples into where and why neural networks fail and give ideas on what can be done.</p>           <p>In the second part of the series we will dive into the mathematics of the neural networks. We will show how neural networks split up the input space (or more exactly chops it up). We will see how the points are remapped in each layer and we will even calculate the final function and see the different constraints. We will be working on small simple fully connected networks with ReLU as the activation function for classification problems. Even so will show that some of the same concepts work with more complex architectures and activation functions. So lets get started!!</p>         </div>       </div>       <hr class=\"m-0\">       <div>        <h2 class=\"mb-5\" style=\"margin-top: 30px\">Neural network as an approximator</h2>          <div class=\"resume-content\">           <p>Since neural networks design was inspired by the human brain, it is easy to forget that at the end of the day a neural network is a mathematical function. Here we will look at neural networks a little bit differently. Instead of looking at the individual neuron and what it has learnt we will take a “zoom out” approach and look at the entire neural network function and how it spans the entire input space. This approach is very input space oriented. This means that we focus allot more on different regions of the input space. To be even more exact we will focus on how the data points are distributed in the input space. This will hopefully give the reader better insights and intuition in neural networks.</p>            <p>So we claim that a neural network is an approximator. What does this mean? This means that the neural network is trying to approximate a function. The question is what function is it trying to approximate? Under what conditions? How is it doing it? What are the limitations? In order to find answers to these questions we have to better understand the problem domain in which we are operating. We will do so with an example.</p>            <p>Let’s say that John is on vacation in Paris. During the day he is snapping away with his camera and during the evening he is dining out so he doesn’t have time to take pictures. After his week long vacation he has a significant number of photos of Paris in the daytime which he decides to share with the world by uploading the pictures online.</p>            <img src=\"\" style=\"width: 100%;padding: 20px\">            <p>Let us look at the photos as a computer would see them. Let’s say John’s camera isn’t great and the photos are 320x240 grayscale images. So each photo is a single 76800 (=320x240) long vector with values between 0 and 255. How would these points look in the input space? If the camera was only 3 pixels long than our input space would be a 3 axis input space. Then each picture would be a single point inside of a 255x255x255 cube in the input space.</p>            <img src=\"\" style=\"width: 100% padding:20px\">            <p>If this is the case, we can imagine that in our case each one of John’s pictures is a single point inside of a 76800 dimensional hyper-cube.</p>            <p>We should notice something important. In total there is a finite number of possible pictures (256⁷⁶⁸⁰⁰ to be exact). Out of all these points some, if not most, of these points are not even real pictures but rather random noise. These are “pictures” that can’t really be taken with John’s camera. In other words if we talk about each picture being a sample in the input space, then we can be sure that it is physically impossible to capture these images (sample these points) using John’s camera.</p>            <p>If we look at just one of the pictures taken by John, as we showed before, this picture is a point in the input space. What if we look at the the points adjacent to the that point, or that are really close to it? We will see that the pictures that they represent only differ a bit from the original picture. A person looking at the pictures will say they are essentially the same picture. Thus if we were to build a neural network we would expect the network to give the same results for all the points around that point (as long as we don’t wonder too far away off). So points that are close enough to each other in the input space are essentially the same picture. The opposite isn’t true. If we have two pictures, where the second picture is the same as the first picture just all the points are moved one pixel to the right. They are essentially the same picture but there points are not necessarily close in the input space.</p>            <p>Lastly we will notice that John is selectively only sampling various regions of the input space. His pictures are sampling Paris in the daytime (probably many pixels will be light high values). These points are probably pretty far away from pictures of Beijing at night (most pixels will be dark low values).</p>            <p>We can split the 256⁷⁶⁸⁰⁰ theoretical pictures into three groups. The first group is regions of the input space which are random noise pictures that a camera can’t produce. These are areas of the input space which will never sample neither during the training nor during the inference stage since it is impossible for a camera to sample them. The remaining areas that hold legal images we can further split into two groups. Firstly is regions of the input space which are close to the pictures John took of Paris during the day. The last group is all the rest of input space. Pictures of Beijing during the night will belong to this group. It will come back later on.</p>         </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">Creating a data set</h2>          <div class=\"resume-content\">            <p>We decide to use John’s pictures to build a neural network that does something. In order to do this, pictures aren’t enough. We need a labelled data set. Let’s say we want to build a neural network is able to classify if there is a cat in the picture. In order to create this data set we hire 1 million people. We ask EACH person to tag EVERY picture if there is a cat in the picture. So we get a data set with 1 million classification result for EVERY one of John’s pictures. The 1 million labels for a single image will not necessarily be the same. This could be due to the fact that someone may have mislabeled the image, but it could be due to the fact that the people we hired may not necessarily agree with each other. Maybe the cat is hidden or it is difficult to determine that there is a cat in the picture. Maybe only the edge of the cat’s tail is visible in the image. In such a case it becomes more of a philosophical question: Is a cat tail consider a cat in the picture?</p>            <img src=\"\" style=\"width: 100%; padding: 20px\">            <p>We could have just as easily asked the million people any other question and created a totally different data set that could be used to teach another neural network something else altogether.</p>            <p>Finally we will use this data set to build a neural network. The purpose of the neural network is to essentially replace the 1 million people. That means that if we have a new picture and feed it through the neural network the answer we should get should be the same as the average of we asked 1 million people if there was a cat in the picture.</p>            <img src=\"\" style=\"width: 100%; padding:20px\">            <p>Let’s dive in a little deeper how we are building this neural network. We take the data set and perform the training using back propagation. So every time we go through all the data points and we are changing the weights of our neural network so that the output of the network will be closer to the data point’s output. We only look at one of John’s pictures. We have 1 million data point entries for that particular input point (picture). All the entries that classify that image as containing a cat will try to pull the output of our neural network up towards 1 for that particular input, and all the entries that classify no cat will try to bring it down to 0. After running back propagation for enough epochs the resulting neural network output for that particular picture in the input should be the average of all the 1 million classifiers. So if 99% of the classifiers agree that there is a cat in the picture the output of the neural network should ideally be around 0.99. In practice this may not be the case and the neural network could still give us good results but ideally this is what we are looking for.</p>            <p>This is all nice, but in real life we can’t have 1 million people tag every image. That is true, but this theoretical example is giving us some basic intuition into what is going on. How is a real data set created? Let’s imagine we have a function that is spread all over the input space. This function answers the following question for each and every possible picture in the input space (which includes 256⁷⁶⁸⁰⁰ possible theoretical pictures). For each picture it answers: “If I were to give THIS image to one of the people doing the classification what is the probability that they would return that there is a cat in the picture?” This function is a very important function. We will eventually see that this in fact is the function that the neural network is trying to emulate, at least in certain parts of the input space.</p>            <p>But how does this function help us create a labeled data set? In order to create the data set, we sample this function at certain points in the input space. In our example these points are John’s pictures. For each input point we get a probability. We use that probability to draw a label for that data point. That is how we eventually build a complete data set. In order to convince you more that this probability function really exists here is another way of looking at how the training data is created. In order to create a label for a certain picture, we randomly pick one of the million people we had labeling the data. We then take the result of their classification as the label for that picture. Randomly drawing a person and using their label is essentially the same as finding the probability of getting a certain label and then drawing a label according to that probability. There are a few constraints on this probabilistic function, in order for it to even be possible to solve the classification problem using neural networks. One of the constraints is that this function has to be a relatively smooth, at least in the areas of the input space where we will be sampling. This is a necessity since if the function is too erratic knowing the labels for a certain point will not help us understand the labels of adjacent points or points which are in close proximity. In such a case we will say that the problem doesn’t generalize well. This is not the only constraint, there are more which we will cover in future posts.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\"style=\"margin-top: 30px\">Using the dataset to approximate</h2>          <div class=\"resume-content\">            <p>We then use this data set to try to approximate this probabilistic function. Since we are assuming that this probabilistic function is relatively smooth, the images, which are scattered in the input space, together with their labels, will help us rebuild this probabilistic function. To reconstruct the function we use a technique called curve fitting (which is what back propagation is essentially doing). We will cover more of this in the following blog post.</p>            <p>I would like to note that the input distribution, the pictures John is taking, is totally independent of the classification and everything else we are doing. John is independent to take any photos he likes regardless of anything. We can already start to see where problems might arise. Let’s say that John continued with his vacation and flew to Beijing, China. While in Beijing he toured one of Beijing’s night markets. As before he took a bunch of pictures and uploaded them to the internet. Let’s say we decided to take these pictures and test our neural network. We have already established that the pictures of Beijing during the night are probably in a very different from the pictures of Paris during the day, and occupy different areas of the input space. What we will see is that the neural network will give really bad result in this part of the input space. The resulting output may even be out of the 0–1 range, meaning we might get a probability that is higher than one or negative which doesn’t make sense. We will understand why this is in the next blog post. But the fact that the neural network give bad results does make sense. From the data the neural network is fed in the training stage, it is theoretically impossible for the neural network to give reliable results outside of its input distribution. Let us explain. The neural network only sees examples of pictures and labels. It doesn’t know the question we asked the people that were doing the classification. The question we used to train the neural network was “is there a cat in the picture?”, but it could have easily been “is there a cat in the picture during the day?”. These two questions would have generated the same data set for John’s Paris day time pictures. If we build a neural network from this data set, and then queried it on Beijing’s night time pictures what answer will the neural network return? Since the neural network has no way of knowing the classification question from the given data, it has no way of returning a correct answer.</p>            <p>This is giving us a clue that contrary to popular belief, the neural network isn’t learning in general what a cat is. Instead the neural network is trying to figure out if a given image is similar to another images it has seen in the training data. It is using these images, which are relatively close to it in the input space in order to try to figure how to classify that point. That is why if we try to classify points that are far away from our input distribution we will find that the neural network doesn’t work well.</p>            <p>If you remember we talked about the input space being split up into three groups: regions of the input space which are random noise, regions which are the same close to the training images and all the rest. In regions that are of random noise, we don’t really care what result our neural network will give us since we assume that we will not be sampling from these regions. Then we had the regions that were not close to the training data. These are data points that are way out of the areas of the input distribution. These are regions where we have to be careful because the network will return a result but we have no way of knowing if the result is correct. Using the neural network to evaluate the function is these regions is called extrapolation and the results are unreliable. For the regions of the input space where we had our training images (areas that are part of the input distribution), we should get decent results since we build the neural network to preform well in these areas. Even so there may be parts of these regions where the neural network may not preform well. This has to do with the input distribution. For instance if there are areas where we have a very small number of points in our input distribution. You can image that train a network with John’s Paris pictures together with a single picture of Beijing’s night market. This new neural network will not, all of a sudden, perform well the Beijing night market pictures.</p>          </div>        </div>          <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">Summing things up</h2>          <div class=\"resume-content\">            <p>We started off with an input space dictated by our features (in our example all the possible pictures). On top of this input space there is an unknown probabilistic function that answers the question “For a given input what is the probability that it will be classified as (input whatever question)?”. This is the function that we ideally want our neural network to emulate, at least for the areas of the input space where we will be sampling. We then sample this function in the points of the input space according to a given input distribution (John’s pictures). For each of these points we draw a label according to this probability given by this function. That is how we derive a data set. We then take this data set and use it to train our neural network hoping that we get back a function that is close enough to the unknown probabilistic function in the areas of the input space of our given input distribution.</p>            <div class=\"subheading mb-5\">See you in the next part. Stay tuned !!</div>          </div>        </div>    </section>",
    "part3": "<section class=\"resume-section   justify-items-center\" id=\"about\">      <div>        <h1>Understanding Neural Networks - Part 1</h1>        <div class=\"subheading mb-5\" style=\"margin-top: 10px\">Get deep into how neural networks really work behind the scenes</div>          <div class=\"resume-content\">            <p>Neural networks have revolutionized how we solve complex problems that have been unsolvable using other techniques. Even though we can get them to work, due to their complex mathematical architecture it is difficult to understand why they work (or more importantly why they don’t work). Google’s Ali Rahimi compared machine learning to alchemy. Even though Yann Lecun strongly disagreed with this statement, he did state in a recent interview “There are all kinds of tricks to get backprop to work, and it’s still a bit of a black art — but now we have a recipe. If you follow the recipe, it’s going to work every time.” In this series of blog articles I will try to explain the inner working of neural networks as to give a better intuition to what is happening behind the scenes.</p>           <p>This series of blog posts will be split into two parts. The first set of blog posts “Understanding Neural Networks” will give a better understanding of the neural network problem domain. We will give a full explanation of what the neural network is doing and under which conditions it is operating. We will do what is seldom done and show that a neural network is actually an approximator. We will show that the neural network is trying to emulate the function that is generating the labels for a set of given data points. Finally we will give examples into where and why neural networks fail and give ideas on what can be done.</p>           <p>In the second part of the series we will dive into the mathematics of the neural networks. We will show how neural networks split up the input space (or more exactly chops it up). We will see how the points are remapped in each layer and we will even calculate the final function and see the different constraints. We will be working on small simple fully connected networks with ReLU as the activation function for classification problems. Even so will show that some of the same concepts work with more complex architectures and activation functions. So lets get started!!</p>         </div>       </div>       <hr class=\"m-0\">       <div>        <h2 class=\"mb-5\" style=\"margin-top: 30px\">Neural network as an approximator</h2>          <div class=\"resume-content\">           <p>Since neural networks design was inspired by the human brain, it is easy to forget that at the end of the day a neural network is a mathematical function. Here we will look at neural networks a little bit differently. Instead of looking at the individual neuron and what it has learnt we will take a “zoom out” approach and look at the entire neural network function and how it spans the entire input space. This approach is very input space oriented. This means that we focus allot more on different regions of the input space. To be even more exact we will focus on how the data points are distributed in the input space. This will hopefully give the reader better insights and intuition in neural networks.</p>            <p>So we claim that a neural network is an approximator. What does this mean? This means that the neural network is trying to approximate a function. The question is what function is it trying to approximate? Under what conditions? How is it doing it? What are the limitations? In order to find answers to these questions we have to better understand the problem domain in which we are operating. We will do so with an example.</p>            <p>Let’s say that John is on vacation in Paris. During the day he is snapping away with his camera and during the evening he is dining out so he doesn’t have time to take pictures. After his week long vacation he has a significant number of photos of Paris in the daytime which he decides to share with the world by uploading the pictures online.</p>            <img src=\"\" style=\"width: 100%;padding: 20px\">            <p>Let us look at the photos as a computer would see them. Let’s say John’s camera isn’t great and the photos are 320x240 grayscale images. So each photo is a single 76800 (=320x240) long vector with values between 0 and 255. How would these points look in the input space? If the camera was only 3 pixels long than our input space would be a 3 axis input space. Then each picture would be a single point inside of a 255x255x255 cube in the input space.</p>            <img src=\"\" style=\"width: 100% padding:20px\">            <p>If this is the case, we can imagine that in our case each one of John’s pictures is a single point inside of a 76800 dimensional hyper-cube.</p>            <p>We should notice something important. In total there is a finite number of possible pictures (256⁷⁶⁸⁰⁰ to be exact). Out of all these points some, if not most, of these points are not even real pictures but rather random noise. These are “pictures” that can’t really be taken with John’s camera. In other words if we talk about each picture being a sample in the input space, then we can be sure that it is physically impossible to capture these images (sample these points) using John’s camera.</p>            <p>If we look at just one of the pictures taken by John, as we showed before, this picture is a point in the input space. What if we look at the the points adjacent to the that point, or that are really close to it? We will see that the pictures that they represent only differ a bit from the original picture. A person looking at the pictures will say they are essentially the same picture. Thus if we were to build a neural network we would expect the network to give the same results for all the points around that point (as long as we don’t wonder too far away off). So points that are close enough to each other in the input space are essentially the same picture. The opposite isn’t true. If we have two pictures, where the second picture is the same as the first picture just all the points are moved one pixel to the right. They are essentially the same picture but there points are not necessarily close in the input space.</p>            <p>Lastly we will notice that John is selectively only sampling various regions of the input space. His pictures are sampling Paris in the daytime (probably many pixels will be light high values). These points are probably pretty far away from pictures of Beijing at night (most pixels will be dark low values).</p>            <p>We can split the 256⁷⁶⁸⁰⁰ theoretical pictures into three groups. The first group is regions of the input space which are random noise pictures that a camera can’t produce. These are areas of the input space which will never sample neither during the training nor during the inference stage since it is impossible for a camera to sample them. The remaining areas that hold legal images we can further split into two groups. Firstly is regions of the input space which are close to the pictures John took of Paris during the day. The last group is all the rest of input space. Pictures of Beijing during the night will belong to this group. It will come back later on.</p>         </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">Creating a data set</h2>          <div class=\"resume-content\">            <p>We decide to use John’s pictures to build a neural network that does something. In order to do this, pictures aren’t enough. We need a labelled data set. Let’s say we want to build a neural network is able to classify if there is a cat in the picture. In order to create this data set we hire 1 million people. We ask EACH person to tag EVERY picture if there is a cat in the picture. So we get a data set with 1 million classification result for EVERY one of John’s pictures. The 1 million labels for a single image will not necessarily be the same. This could be due to the fact that someone may have mislabeled the image, but it could be due to the fact that the people we hired may not necessarily agree with each other. Maybe the cat is hidden or it is difficult to determine that there is a cat in the picture. Maybe only the edge of the cat’s tail is visible in the image. In such a case it becomes more of a philosophical question: Is a cat tail consider a cat in the picture?</p>            <img src=\"\" style=\"width: 100%; padding: 20px\">            <p>We could have just as easily asked the million people any other question and created a totally different data set that could be used to teach another neural network something else altogether.</p>            <p>Finally we will use this data set to build a neural network. The purpose of the neural network is to essentially replace the 1 million people. That means that if we have a new picture and feed it through the neural network the answer we should get should be the same as the average of we asked 1 million people if there was a cat in the picture.</p>            <img src=\"\" style=\"width: 100%; padding:20px\">            <p>Let’s dive in a little deeper how we are building this neural network. We take the data set and perform the training using back propagation. So every time we go through all the data points and we are changing the weights of our neural network so that the output of the network will be closer to the data point’s output. We only look at one of John’s pictures. We have 1 million data point entries for that particular input point (picture). All the entries that classify that image as containing a cat will try to pull the output of our neural network up towards 1 for that particular input, and all the entries that classify no cat will try to bring it down to 0. After running back propagation for enough epochs the resulting neural network output for that particular picture in the input should be the average of all the 1 million classifiers. So if 99% of the classifiers agree that there is a cat in the picture the output of the neural network should ideally be around 0.99. In practice this may not be the case and the neural network could still give us good results but ideally this is what we are looking for.</p>            <p>This is all nice, but in real life we can’t have 1 million people tag every image. That is true, but this theoretical example is giving us some basic intuition into what is going on. How is a real data set created? Let’s imagine we have a function that is spread all over the input space. This function answers the following question for each and every possible picture in the input space (which includes 256⁷⁶⁸⁰⁰ possible theoretical pictures). For each picture it answers: “If I were to give THIS image to one of the people doing the classification what is the probability that they would return that there is a cat in the picture?” This function is a very important function. We will eventually see that this in fact is the function that the neural network is trying to emulate, at least in certain parts of the input space.</p>            <p>But how does this function help us create a labeled data set? In order to create the data set, we sample this function at certain points in the input space. In our example these points are John’s pictures. For each input point we get a probability. We use that probability to draw a label for that data point. That is how we eventually build a complete data set. In order to convince you more that this probability function really exists here is another way of looking at how the training data is created. In order to create a label for a certain picture, we randomly pick one of the million people we had labeling the data. We then take the result of their classification as the label for that picture. Randomly drawing a person and using their label is essentially the same as finding the probability of getting a certain label and then drawing a label according to that probability. There are a few constraints on this probabilistic function, in order for it to even be possible to solve the classification problem using neural networks. One of the constraints is that this function has to be a relatively smooth, at least in the areas of the input space where we will be sampling. This is a necessity since if the function is too erratic knowing the labels for a certain point will not help us understand the labels of adjacent points or points which are in close proximity. In such a case we will say that the problem doesn’t generalize well. This is not the only constraint, there are more which we will cover in future posts.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\"style=\"margin-top: 30px\">Using the dataset to approximate</h2>          <div class=\"resume-content\">            <p>We then use this data set to try to approximate this probabilistic function. Since we are assuming that this probabilistic function is relatively smooth, the images, which are scattered in the input space, together with their labels, will help us rebuild this probabilistic function. To reconstruct the function we use a technique called curve fitting (which is what back propagation is essentially doing). We will cover more of this in the following blog post.</p>            <p>I would like to note that the input distribution, the pictures John is taking, is totally independent of the classification and everything else we are doing. John is independent to take any photos he likes regardless of anything. We can already start to see where problems might arise. Let’s say that John continued with his vacation and flew to Beijing, China. While in Beijing he toured one of Beijing’s night markets. As before he took a bunch of pictures and uploaded them to the internet. Let’s say we decided to take these pictures and test our neural network. We have already established that the pictures of Beijing during the night are probably in a very different from the pictures of Paris during the day, and occupy different areas of the input space. What we will see is that the neural network will give really bad result in this part of the input space. The resulting output may even be out of the 0–1 range, meaning we might get a probability that is higher than one or negative which doesn’t make sense. We will understand why this is in the next blog post. But the fact that the neural network give bad results does make sense. From the data the neural network is fed in the training stage, it is theoretically impossible for the neural network to give reliable results outside of its input distribution. Let us explain. The neural network only sees examples of pictures and labels. It doesn’t know the question we asked the people that were doing the classification. The question we used to train the neural network was “is there a cat in the picture?”, but it could have easily been “is there a cat in the picture during the day?”. These two questions would have generated the same data set for John’s Paris day time pictures. If we build a neural network from this data set, and then queried it on Beijing’s night time pictures what answer will the neural network return? Since the neural network has no way of knowing the classification question from the given data, it has no way of returning a correct answer.</p>            <p>This is giving us a clue that contrary to popular belief, the neural network isn’t learning in general what a cat is. Instead the neural network is trying to figure out if a given image is similar to another images it has seen in the training data. It is using these images, which are relatively close to it in the input space in order to try to figure how to classify that point. That is why if we try to classify points that are far away from our input distribution we will find that the neural network doesn’t work well.</p>            <p>If you remember we talked about the input space being split up into three groups: regions of the input space which are random noise, regions which are the same close to the training images and all the rest. In regions that are of random noise, we don’t really care what result our neural network will give us since we assume that we will not be sampling from these regions. Then we had the regions that were not close to the training data. These are data points that are way out of the areas of the input distribution. These are regions where we have to be careful because the network will return a result but we have no way of knowing if the result is correct. Using the neural network to evaluate the function is these regions is called extrapolation and the results are unreliable. For the regions of the input space where we had our training images (areas that are part of the input distribution), we should get decent results since we build the neural network to preform well in these areas. Even so there may be parts of these regions where the neural network may not preform well. This has to do with the input distribution. For instance if there are areas where we have a very small number of points in our input distribution. You can image that train a network with John’s Paris pictures together with a single picture of Beijing’s night market. This new neural network will not, all of a sudden, perform well the Beijing night market pictures.</p>          </div>        </div>          <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">Summing things up</h2>          <div class=\"resume-content\">            <p>We started off with an input space dictated by our features (in our example all the possible pictures). On top of this input space there is an unknown probabilistic function that answers the question “For a given input what is the probability that it will be classified as (input whatever question)?”. This is the function that we ideally want our neural network to emulate, at least for the areas of the input space where we will be sampling. We then sample this function in the points of the input space according to a given input distribution (John’s pictures). For each of these points we draw a label according to this probability given by this function. That is how we derive a data set. We then take this data set and use it to train our neural network hoping that we get back a function that is close enough to the unknown probabilistic function in the areas of the input space of our given input distribution.</p>            <div class=\"subheading mb-5\">See you in the next part. Stay tuned !!</div>          </div>        </div>    </section>",
    "part4": "<section class=\"resume-section   justify-items-center\" id=\"about\">      <div>        <h1>Understanding Neural Networks - Part 1</h1>        <div class=\"subheading mb-5\" style=\"margin-top: 10px\">Get deep into how neural networks really work behind the scenes</div>          <div class=\"resume-content\">            <p>Neural networks have revolutionized how we solve complex problems that have been unsolvable using other techniques. Even though we can get them to work, due to their complex mathematical architecture it is difficult to understand why they work (or more importantly why they don’t work). Google’s Ali Rahimi compared machine learning to alchemy. Even though Yann Lecun strongly disagreed with this statement, he did state in a recent interview “There are all kinds of tricks to get backprop to work, and it’s still a bit of a black art — but now we have a recipe. If you follow the recipe, it’s going to work every time.” In this series of blog articles I will try to explain the inner working of neural networks as to give a better intuition to what is happening behind the scenes.</p>           <p>This series of blog posts will be split into two parts. The first set of blog posts “Understanding Neural Networks” will give a better understanding of the neural network problem domain. We will give a full explanation of what the neural network is doing and under which conditions it is operating. We will do what is seldom done and show that a neural network is actually an approximator. We will show that the neural network is trying to emulate the function that is generating the labels for a set of given data points. Finally we will give examples into where and why neural networks fail and give ideas on what can be done.</p>           <p>In the second part of the series we will dive into the mathematics of the neural networks. We will show how neural networks split up the input space (or more exactly chops it up). We will see how the points are remapped in each layer and we will even calculate the final function and see the different constraints. We will be working on small simple fully connected networks with ReLU as the activation function for classification problems. Even so will show that some of the same concepts work with more complex architectures and activation functions. So lets get started!!</p>         </div>       </div>       <hr class=\"m-0\">       <div>        <h2 class=\"mb-5\" style=\"margin-top: 30px\">Neural network as an approximator</h2>          <div class=\"resume-content\">           <p>Since neural networks design was inspired by the human brain, it is easy to forget that at the end of the day a neural network is a mathematical function. Here we will look at neural networks a little bit differently. Instead of looking at the individual neuron and what it has learnt we will take a “zoom out” approach and look at the entire neural network function and how it spans the entire input space. This approach is very input space oriented. This means that we focus allot more on different regions of the input space. To be even more exact we will focus on how the data points are distributed in the input space. This will hopefully give the reader better insights and intuition in neural networks.</p>            <p>So we claim that a neural network is an approximator. What does this mean? This means that the neural network is trying to approximate a function. The question is what function is it trying to approximate? Under what conditions? How is it doing it? What are the limitations? In order to find answers to these questions we have to better understand the problem domain in which we are operating. We will do so with an example.</p>            <p>Let’s say that John is on vacation in Paris. During the day he is snapping away with his camera and during the evening he is dining out so he doesn’t have time to take pictures. After his week long vacation he has a significant number of photos of Paris in the daytime which he decides to share with the world by uploading the pictures online.</p>            <img src=\"\" style=\"width: 100%;padding: 20px\">            <p>Let us look at the photos as a computer would see them. Let’s say John’s camera isn’t great and the photos are 320x240 grayscale images. So each photo is a single 76800 (=320x240) long vector with values between 0 and 255. How would these points look in the input space? If the camera was only 3 pixels long than our input space would be a 3 axis input space. Then each picture would be a single point inside of a 255x255x255 cube in the input space.</p>            <img src=\"\" style=\"width: 100% padding:20px\">            <p>If this is the case, we can imagine that in our case each one of John’s pictures is a single point inside of a 76800 dimensional hyper-cube.</p>            <p>We should notice something important. In total there is a finite number of possible pictures (256⁷⁶⁸⁰⁰ to be exact). Out of all these points some, if not most, of these points are not even real pictures but rather random noise. These are “pictures” that can’t really be taken with John’s camera. In other words if we talk about each picture being a sample in the input space, then we can be sure that it is physically impossible to capture these images (sample these points) using John’s camera.</p>            <p>If we look at just one of the pictures taken by John, as we showed before, this picture is a point in the input space. What if we look at the the points adjacent to the that point, or that are really close to it? We will see that the pictures that they represent only differ a bit from the original picture. A person looking at the pictures will say they are essentially the same picture. Thus if we were to build a neural network we would expect the network to give the same results for all the points around that point (as long as we don’t wonder too far away off). So points that are close enough to each other in the input space are essentially the same picture. The opposite isn’t true. If we have two pictures, where the second picture is the same as the first picture just all the points are moved one pixel to the right. They are essentially the same picture but there points are not necessarily close in the input space.</p>            <p>Lastly we will notice that John is selectively only sampling various regions of the input space. His pictures are sampling Paris in the daytime (probably many pixels will be light high values). These points are probably pretty far away from pictures of Beijing at night (most pixels will be dark low values).</p>            <p>We can split the 256⁷⁶⁸⁰⁰ theoretical pictures into three groups. The first group is regions of the input space which are random noise pictures that a camera can’t produce. These are areas of the input space which will never sample neither during the training nor during the inference stage since it is impossible for a camera to sample them. The remaining areas that hold legal images we can further split into two groups. Firstly is regions of the input space which are close to the pictures John took of Paris during the day. The last group is all the rest of input space. Pictures of Beijing during the night will belong to this group. It will come back later on.</p>         </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">Creating a data set</h2>          <div class=\"resume-content\">            <p>We decide to use John’s pictures to build a neural network that does something. In order to do this, pictures aren’t enough. We need a labelled data set. Let’s say we want to build a neural network is able to classify if there is a cat in the picture. In order to create this data set we hire 1 million people. We ask EACH person to tag EVERY picture if there is a cat in the picture. So we get a data set with 1 million classification result for EVERY one of John’s pictures. The 1 million labels for a single image will not necessarily be the same. This could be due to the fact that someone may have mislabeled the image, but it could be due to the fact that the people we hired may not necessarily agree with each other. Maybe the cat is hidden or it is difficult to determine that there is a cat in the picture. Maybe only the edge of the cat’s tail is visible in the image. In such a case it becomes more of a philosophical question: Is a cat tail consider a cat in the picture?</p>            <img src=\"\" style=\"width: 100%; padding: 20px\">            <p>We could have just as easily asked the million people any other question and created a totally different data set that could be used to teach another neural network something else altogether.</p>            <p>Finally we will use this data set to build a neural network. The purpose of the neural network is to essentially replace the 1 million people. That means that if we have a new picture and feed it through the neural network the answer we should get should be the same as the average of we asked 1 million people if there was a cat in the picture.</p>            <img src=\"\" style=\"width: 100%; padding:20px\">            <p>Let’s dive in a little deeper how we are building this neural network. We take the data set and perform the training using back propagation. So every time we go through all the data points and we are changing the weights of our neural network so that the output of the network will be closer to the data point’s output. We only look at one of John’s pictures. We have 1 million data point entries for that particular input point (picture). All the entries that classify that image as containing a cat will try to pull the output of our neural network up towards 1 for that particular input, and all the entries that classify no cat will try to bring it down to 0. After running back propagation for enough epochs the resulting neural network output for that particular picture in the input should be the average of all the 1 million classifiers. So if 99% of the classifiers agree that there is a cat in the picture the output of the neural network should ideally be around 0.99. In practice this may not be the case and the neural network could still give us good results but ideally this is what we are looking for.</p>            <p>This is all nice, but in real life we can’t have 1 million people tag every image. That is true, but this theoretical example is giving us some basic intuition into what is going on. How is a real data set created? Let’s imagine we have a function that is spread all over the input space. This function answers the following question for each and every possible picture in the input space (which includes 256⁷⁶⁸⁰⁰ possible theoretical pictures). For each picture it answers: “If I were to give THIS image to one of the people doing the classification what is the probability that they would return that there is a cat in the picture?” This function is a very important function. We will eventually see that this in fact is the function that the neural network is trying to emulate, at least in certain parts of the input space.</p>            <p>But how does this function help us create a labeled data set? In order to create the data set, we sample this function at certain points in the input space. In our example these points are John’s pictures. For each input point we get a probability. We use that probability to draw a label for that data point. That is how we eventually build a complete data set. In order to convince you more that this probability function really exists here is another way of looking at how the training data is created. In order to create a label for a certain picture, we randomly pick one of the million people we had labeling the data. We then take the result of their classification as the label for that picture. Randomly drawing a person and using their label is essentially the same as finding the probability of getting a certain label and then drawing a label according to that probability. There are a few constraints on this probabilistic function, in order for it to even be possible to solve the classification problem using neural networks. One of the constraints is that this function has to be a relatively smooth, at least in the areas of the input space where we will be sampling. This is a necessity since if the function is too erratic knowing the labels for a certain point will not help us understand the labels of adjacent points or points which are in close proximity. In such a case we will say that the problem doesn’t generalize well. This is not the only constraint, there are more which we will cover in future posts.</p>          </div>       </div>       <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\"style=\"margin-top: 30px\">Using the dataset to approximate</h2>          <div class=\"resume-content\">            <p>We then use this data set to try to approximate this probabilistic function. Since we are assuming that this probabilistic function is relatively smooth, the images, which are scattered in the input space, together with their labels, will help us rebuild this probabilistic function. To reconstruct the function we use a technique called curve fitting (which is what back propagation is essentially doing). We will cover more of this in the following blog post.</p>            <p>I would like to note that the input distribution, the pictures John is taking, is totally independent of the classification and everything else we are doing. John is independent to take any photos he likes regardless of anything. We can already start to see where problems might arise. Let’s say that John continued with his vacation and flew to Beijing, China. While in Beijing he toured one of Beijing’s night markets. As before he took a bunch of pictures and uploaded them to the internet. Let’s say we decided to take these pictures and test our neural network. We have already established that the pictures of Beijing during the night are probably in a very different from the pictures of Paris during the day, and occupy different areas of the input space. What we will see is that the neural network will give really bad result in this part of the input space. The resulting output may even be out of the 0–1 range, meaning we might get a probability that is higher than one or negative which doesn’t make sense. We will understand why this is in the next blog post. But the fact that the neural network give bad results does make sense. From the data the neural network is fed in the training stage, it is theoretically impossible for the neural network to give reliable results outside of its input distribution. Let us explain. The neural network only sees examples of pictures and labels. It doesn’t know the question we asked the people that were doing the classification. The question we used to train the neural network was “is there a cat in the picture?”, but it could have easily been “is there a cat in the picture during the day?”. These two questions would have generated the same data set for John’s Paris day time pictures. If we build a neural network from this data set, and then queried it on Beijing’s night time pictures what answer will the neural network return? Since the neural network has no way of knowing the classification question from the given data, it has no way of returning a correct answer.</p>            <p>This is giving us a clue that contrary to popular belief, the neural network isn’t learning in general what a cat is. Instead the neural network is trying to figure out if a given image is similar to another images it has seen in the training data. It is using these images, which are relatively close to it in the input space in order to try to figure how to classify that point. That is why if we try to classify points that are far away from our input distribution we will find that the neural network doesn’t work well.</p>            <p>If you remember we talked about the input space being split up into three groups: regions of the input space which are random noise, regions which are the same close to the training images and all the rest. In regions that are of random noise, we don’t really care what result our neural network will give us since we assume that we will not be sampling from these regions. Then we had the regions that were not close to the training data. These are data points that are way out of the areas of the input distribution. These are regions where we have to be careful because the network will return a result but we have no way of knowing if the result is correct. Using the neural network to evaluate the function is these regions is called extrapolation and the results are unreliable. For the regions of the input space where we had our training images (areas that are part of the input distribution), we should get decent results since we build the neural network to preform well in these areas. Even so there may be parts of these regions where the neural network may not preform well. This has to do with the input distribution. For instance if there are areas where we have a very small number of points in our input distribution. You can image that train a network with John’s Paris pictures together with a single picture of Beijing’s night market. This new neural network will not, all of a sudden, perform well the Beijing night market pictures.</p>          </div>        </div>          <hr class=\"m-0\">       <div>         <h2 class=\"mb-5\" style=\"margin-top: 30px\">Summing things up</h2>          <div class=\"resume-content\">            <p>We started off with an input space dictated by our features (in our example all the possible pictures). On top of this input space there is an unknown probabilistic function that answers the question “For a given input what is the probability that it will be classified as (input whatever question)?”. This is the function that we ideally want our neural network to emulate, at least for the areas of the input space where we will be sampling. We then sample this function in the points of the input space according to a given input distribution (John’s pictures). For each of these points we draw a label according to this probability given by this function. That is how we derive a data set. We then take this data set and use it to train our neural network hoping that we get back a function that is close enough to the unknown probabilistic function in the areas of the input space of our given input distribution.</p>            <div class=\"subheading mb-5\">See you in the next part. Stay tuned !!</div>          </div>        </div>    </section>"
    }

course_collection.insert_one(course_post1)


image11 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image1.png?alt=media&token=828daa1f-8ae9-4cdf-ae53-d4b889a17975"
image12 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image2.png?alt=media&token=483ad655-8010-48e1-8818-a3508c740f7d"
image13 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image3.png?alt=media&token=17312747-7a20-46a3-9780-2582521d9ebc"
image14 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image4.png?alt=media&token=f49a8da1-4ec5-41a4-9e1d-917bdf6af76d"

image21 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image21.png?alt=media&token=da91d0b2-fffc-4e12-bf1a-21cfabdc2b84"
image22 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image22.png?alt=media&token=7fc32c3c-887c-49d5-a8ad-f27c603de374"
image23 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image23.png?alt=media&token=25a356ea-5987-4e4a-9d00-1e2196ef6472"
image24 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image24.png?alt=media&token=f25a522c-96c1-4210-8a05-35b683267f4a"
image25 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image25.png?alt=media&token=b43ea23b-feff-4797-bb8f-c841776656a7"
image26 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image26.png?alt=media&token=e11d2cb3-9758-4b0b-9ff5-c27c02c6f417"
image27 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image27.png?alt=media&token=7a11223c-8bd2-40a2-ba39-23a3ea27f072"
image28 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image28.png?alt=media&token=b5bdbad0-6bb7-4513-801f-ae4128d6884c"

image31 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image31.png?alt=media&token=201b4d86-53a8-4140-8af8-25d2c7130ddc"
image32 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image32.png?alt=media&token=a2aca1a1-21c4-4e75-80bb-0e1d674ab13b"
image33 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image33.png?alt=media&token=9dab971e-8820-46c9-8e09-0359ba42bbce"
image34 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image34.png?alt=media&token=e61ef41d-a955-48c6-b950-7e43aad44113"
image35 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image35.png?alt=media&token=ef6fce36-638a-4e5d-ad5e-77d8b48f7975"
image36 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image36.png?alt=media&token=76e38dc2-2771-4c26-850b-e749598834ef"
image37 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image37.png?alt=media&token=d7f4083f-5120-4fc1-bbf8-253616d7ac40"
image38 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image38.png?alt=media&token=ba186d2c-204c-4361-bc38-7e508a61d41b"
image39 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image39.png?alt=media&token=797cc28d-baa5-4f74-b2e9-01c33041ef1f"

image41 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image41.png?alt=media&token=2caf4a0f-3038-488e-b197-4984b4451000"

# image_collection.insert_one({
#     "_id":"ML001",
#     "part1":[image11, image12, image13, image14],
#     "part2":[image21, image22, image23, image24, image25, image26, image27, image28],
#     "part3":[image31, image32, image33, image34, image35, image36, image37, image38, image39],
#     "part4":[image41]
#     })
"""



image11 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image1.png?alt=media&token=828daa1f-8ae9-4cdf-ae53-d4b889a17975"
image12 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image2.png?alt=media&token=483ad655-8010-48e1-8818-a3508c740f7d"
image13 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image3.png?alt=media&token=17312747-7a20-46a3-9780-2582521d9ebc"
image14 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image4.png?alt=media&token=f49a8da1-4ec5-41a4-9e1d-917bdf6af76d"

image21 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image21.png?alt=media&token=da91d0b2-fffc-4e12-bf1a-21cfabdc2b84"
image22 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image22.png?alt=media&token=7fc32c3c-887c-49d5-a8ad-f27c603de374"
image23 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image23.png?alt=media&token=25a356ea-5987-4e4a-9d00-1e2196ef6472"
image24 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image24.png?alt=media&token=f25a522c-96c1-4210-8a05-35b683267f4a"
image25 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image25.png?alt=media&token=b43ea23b-feff-4797-bb8f-c841776656a7"
image26 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image26.png?alt=media&token=e11d2cb3-9758-4b0b-9ff5-c27c02c6f417"
image27 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image27.png?alt=media&token=7a11223c-8bd2-40a2-ba39-23a3ea27f072"
image28 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image28.png?alt=media&token=b5bdbad0-6bb7-4513-801f-ae4128d6884c"

image31 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image31.png?alt=media&token=201b4d86-53a8-4140-8af8-25d2c7130ddc"
image32 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image32.png?alt=media&token=a2aca1a1-21c4-4e75-80bb-0e1d674ab13b"
image33 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image33.png?alt=media&token=9dab971e-8820-46c9-8e09-0359ba42bbce"
image34 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image34.png?alt=media&token=e61ef41d-a955-48c6-b950-7e43aad44113"
image35 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image35.png?alt=media&token=ef6fce36-638a-4e5d-ad5e-77d8b48f7975"
image36 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image36.png?alt=media&token=76e38dc2-2771-4c26-850b-e749598834ef"
image37 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image37.png?alt=media&token=d7f4083f-5120-4fc1-bbf8-253616d7ac40"
image38 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image38.png?alt=media&token=ba186d2c-204c-4361-bc38-7e508a61d41b"
image39 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image39.png?alt=media&token=797cc28d-baa5-4f74-b2e9-01c33041ef1f"

image41 = "https://firebasestorage.googleapis.com/v0/b/androidcloneuber-1524d.appspot.com/o/image41.png?alt=media&token=2caf4a0f-3038-488e-b197-4984b4451000"

cluster = MongoClient("mongodb://dinesh_bhuttu:Dinesh123@cluster0-shard-00-00-qwwij.mongodb.net:27017,cluster0-shard-00-01-qwwij.mongodb.net:27017,cluster0-shard-00-02-qwwij.mongodb.net:27017/test?ssl=true&replicaSet=Cluster0-shard-0&authSource=admin&retryWrites=true&w=majority")
db = cluster["100Marks"]
metadata_collection = db["Course_metadata"]
image_collection = db["ImageCollection"]
course_collection = db["Course"]


post1 = {
    "_id":"ML002",
    "course_name":"Deep Learning with Keras - for dummies",
    "course_description":"In this series of blog articles I will try to explain the inner working of neural networks as to give a better intuition to what is happening behind the scenes.",
    "author":"DV Anudeep",
    "rating":0,
    "viewers":0,
    "reviews":[],
    "duration":3,
    "part1":{"summ":"In this post, we are introduced to a super high level wrapper around tensorflow named KERAS, which can be used by lazy ML practitioners to show off their ML skills",
             "title":"Keras for Dummies - Part 1"},
    "part2":{"summ":"We now start off with the math related to neural networks. And how Keras makes it easy for lazy coders to show they know everything without knowing a bit!",
             "title":"Keras for Dummies - Part 2"},
    "part3":{"summ":"In this part, we train CNNs using Keras!!",
             "title":"Keras for Dummies - Part 3"},
    "part4":{"summ":"To wrap up, we will go through some of the most popular deep learning applications, coded in Keras!!",
             "title":"Keras for Dummies - Part 4"}
 }



post2 = {
    "_id":"ML003",
    "course_name":"Support Vector Machines - Demystified",
    "course_description":"This series has been carefully designed to neatly explain all the intricate details involving SVMs and it's important use cases",
    "author":"DV Anudeep",
    "rating":0,
    "viewers":0,
    "reviews":[],
    "duration":3,
    "part1":{"summ":"In this post, we are introduced to a super high level wrapper around tensorflow named KERAS, which can be used by lazy ML practitioners to show off their ML skills",
             "title":"SVMs Demystified - Part 1"},
    "part2":{"summ":"We now start off with the math related to neural networks. And how Keras makes it easy for lazy coders to show they know everything without knowing a bit!",
             "title":"SVMs Demystified - Part 2"},
    "part3":{"summ":"In this part, we train CNNs using Keras!!",
             "title":"SVMs Demystified - Part 3"},
    "part4":{"summ":"To wrap up, we will go through some of the most popular deep learning applications, coded in Keras!!",
             "title":"SVMs Demystified - Part 4"}
 }


post3 = {
    "_id":"ML004",
    "course_name":"Generative Adversarial Networks (GANs) - Demystified",
    "course_description":"This series has been carefully designed to neatly explain all the intricate details involving SVMs and it's important use cases",
    "author":"DV Anudeep",
    "rating":0,
    "viewers":0,
    "reviews":[],
    "duration":3,
    "part1":{"summ":"In this post, we are introduced to a super high level wrapper around tensorflow named KERAS, which can be used by lazy ML practitioners to show off their ML skills",
             "title":"Generative Adversarial Networks (GANs) Demystified- Part 1"},
    "part2":{"summ":"We now start off with the math related to neural networks. And how Keras makes it easy for lazy coders to show they know everything without knowing a bit!",
             "title":"Generative Adversarial Networks (GANs) Demystified - Part 2"},
    "part3":{"summ":"In this part, we train CNNs using Keras!!",
             "title":"Generative Adversarial Networks (GANs) Demystified - Part 3"},
    "part4":{"summ":"To wrap up, we will go through some of the most popular deep learning applications, coded in Keras!!",
             "title":"Generative Adversarial Networks (GANs) Demystified - Part 4"}
 }


course_post2 = {
    "_id":"ML002",
    "part1":"",
    "part2":"",
    "part3":"",
    "part4":""
}

course_post3 = {
    "_id":"ML003",
    "part1":"",
    "part2":"",
    "part3":"",
    "part4":""
}

course_post4 = {
    "_id":"ML004",
    "part1":"",
    "part2":"",
    "part3":"",
    "part4":""
}



metadata_collection.insert_one(post1)
metadata_collection.insert_one(post2)
metadata_collection.insert_one(post3)


course_collection.insert_one(course_post2)
course_collection.insert_one(course_post3)
course_collection.insert_one(course_post4)


image_collection.insert_one({
    "_id":"ML002",
    "part1":[image11, image12, image13, image14],
    "part2":[image21, image22, image23, image24, image25, image26, image27, image28],
    "part3":[image31, image32, image33, image34, image35, image36, image37, image38, image39],
    "part4":[image41]
    })

image_collection.insert_one({
    "_id":"ML003",
    "part1":[image11, image12, image13, image14],
    "part2":[image21, image22, image23, image24, image25, image26, image27, image28],
    "part3":[image31, image32, image33, image34, image35, image36, image37, image38, image39],
    "part4":[image41]
    })

image_collection.insert_one({
    "_id":"ML004",
    "part1":[image11, image12, image13, image14],
    "part2":[image21, image22, image23, image24, image25, image26, image27, image28],
    "part3":[image31, image32, image33, image34, image35, image36, image37, image38, image39],
    "part4":[image41]
    })

